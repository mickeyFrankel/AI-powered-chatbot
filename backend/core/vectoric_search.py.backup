import os
from dotenv import load_dotenv
load_dotenv()

# Suppress ChromaDB telemetry warnings
os.environ['ANONYMIZED_TELEMETRY'] = 'False'
import sys
import io
import logging

# Suppress ChromaDB logging  
logging.getLogger('chromadb').setLevel(logging.ERROR)
logging.getLogger('chromadb.telemetry').setLevel(logging.CRITICAL)

import re, unicodedata
import pandas as pd
import numpy as np
import chromadb

# Patch chromadb's telemetry to suppress errors
try:
    import chromadb.telemetry.posthog as posthog
    original_capture = posthog.Posthog.capture
    posthog.Posthog.capture = lambda *args, **kwargs: None
except:
    pass
import json
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer

# LangChain imports
try:
    from langchain_core.tools import tool
except ImportError:
    from langchain.tools import tool

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import AIMessage, HumanMessage

try:
    from langchain.agents import create_openai_functions_agent, AgentExecutor
except ImportError:
    try:
        from langchain.agents.openai_functions_agent.base import create_openai_functions_agent
        from langchain.agents.agent import AgentExecutor
    except ImportError:
        # Fallback for older versions
        create_openai_functions_agent = None
        AgentExecutor = None

import docx
from openpyxl import load_workbook
import PyPDF2
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from collections import Counter
import warnings

from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances

# Fuzzy matching for typo tolerance
try:
    from rapidfuzz import fuzz, process
    HAS_FUZZY = True
except ImportError:
    HAS_FUZZY = False
    print("âš ï¸  rapidfuzz not installed. Fuzzy search disabled.")
    print("   Install with: pip install rapidfuzz")

warnings.filterwarnings('ignore')


class VectorDBQASystem:
    """Base VectorDB system - keep unchanged"""
    def __init__(self, persist_directory: str = "./chroma_db", model_name: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"):
        """
        Initialize the VectorDB Q&A System with Hebrew support
        
        Args:
            persist_directory: Directory to persist ChromaDB data
            model_name: Sentence transformer model (supports Hebrew)
        """
        self.persist_directory = persist_directory
        self.model_name = model_name
        
        # Initialize embedding model (supports Hebrew)
        print(f"Loading embedding model: {model_name}")
        self.embedding_model = SentenceTransformer(model_name)
        
        # Initialize ChromaDB client
        self.client = chromadb.PersistentClient(
            path=persist_directory,
            settings=Settings(
                anonymized_telemetry=False,
                allow_reset=True
            )
        )
        
        # Create or get collection
        self.collection_name = "documents"
        try:
            self.collection = self.client.get_collection(self.collection_name)
            print(f"Loaded existing collection: {self.collection_name}")
        except:
            self.collection = self.client.create_collection(
                name=self.collection_name,
                metadata={"hnsw:space": "cosine"}
            )
            print(f"Created new collection: {self.collection_name}")
    
    def reset_database(self):
        """Reset database by deleting and recreating collection"""
        if hasattr(self, 'chat_history'):
            self.chat_history = []
        
        try:
            self.client.delete_collection(self.collection_name)
        except:
            pass  # Collection might not exist
        
        self.collection = self.client.create_collection(
            name=self.collection_name,
            metadata={"hnsw:space": "cosine"}
        )
    
    def _preprocess_dataframe(self, df: pd.DataFrame, source_name: str = "") -> pd.DataFrame:
        """Clean and optimize DataFrame before ingestion"""
        print(f"\nðŸ§¹ Preprocessing {source_name}...")
        print(f"   Initial: {len(df)} rows Ã— {len(df.columns)} columns")
        
        original_cols = len(df.columns)
        original_rows = len(df)
        
        # 1. Remove completely empty columns (100% null)
        empty_cols = df.columns[df.isnull().all()].tolist()
        if empty_cols:
            df = df.drop(columns=empty_cols)
            print(f"   âœ‚ï¸  Removed {len(empty_cols)} empty columns: {empty_cols[:3]}..." if len(empty_cols) > 3 else f"   âœ‚ï¸  Removed {len(empty_cols)} empty columns: {empty_cols}")
        
        # 2. Remove sparse columns (>95% empty) BUT protect important fields
        sparse_threshold = 0.95
        important_patterns = ['phone', 'mobile', 'tel', 'email', 'mail', 'address', 'addr']
        sparse_cols = []
        
        for col in df.columns:
            null_ratio = df[col].isnull().sum() / len(df)
            if null_ratio > sparse_threshold:
                # Check if it's an important field that should be kept
                col_lower = col.lower()
                is_important = any(pattern in col_lower for pattern in important_patterns)
                if not is_important:
                    sparse_cols.append(col)
        
        if sparse_cols:
            df = df.drop(columns=sparse_cols)
            print(f"   âœ‚ï¸  Removed {len(sparse_cols)} sparse columns (>95% empty)")
        
        # 3. Remove low-value metadata columns
        low_value_patterns = ['id', 'uuid', 'guid', 'key', 'index', 'row_num', 'created_at', 'updated_at', 'timestamp', 'date_added', 'last_modified']
        metadata_cols = []
        for col in df.columns:
            col_lower = col.lower()
            if any(pattern in col_lower for pattern in low_value_patterns):
                # Keep if it looks like a phone ("mobile_id" might be "mobile identifier")
                if 'phone' not in col_lower and 'mobile' not in col_lower and 'tel' not in col_lower:
                    metadata_cols.append(col)
        
        if metadata_cols:
            df = df.drop(columns=metadata_cols)
            print(f"   âœ‚ï¸  Removed {len(metadata_cols)} metadata columns")
        
        # 4. Trim whitespace from all string columns
        for col in df.columns:
            if df[col].dtype == 'object':  # String columns
                df[col] = df[col].apply(lambda x: x.strip() if isinstance(x, str) else x)
        
        # 5. Remove duplicate rows (exact duplicates)
        duplicates = df.duplicated().sum()
        if duplicates > 0:
            df = df.drop_duplicates()
            print(f"   âœ‚ï¸  Removed {duplicates} duplicate rows")
        
        # 6. Consolidate phone columns (prioritize 'value' over 'type/label')
        phone_cols = [col for col in df.columns if 'phone' in col.lower() or 'mobile' in col.lower() or 'tel' in col.lower()]
        if len(phone_cols) > 1:
            # Sort to prioritize columns with 'value', 'number', or numeric data
            value_cols = [col for col in phone_cols if 'value' in col.lower() or 'number' in col.lower()]
            type_cols = [col for col in phone_cols if 'type' in col.lower() or 'label' in col.lower()]
            other_cols = [col for col in phone_cols if col not in value_cols and col not in type_cols]
            
            # Prioritize: value columns > other columns > type columns (last)
            sorted_phone_cols = value_cols + other_cols + type_cols
            
            # Create single 'phone' column, filtering out non-numeric values
            def get_phone_value(row):
                for col in sorted_phone_cols:
                    val = row[col]
                    if pd.notna(val) and str(val).strip():
                        val_str = str(val).strip()
                        # Skip if it looks like a label (e.g., "Mobile", "× ×™×™×“")
                        # Phone numbers must contain at least one digit
                        if any(c.isdigit() for c in val_str):
                            return val
                return None
            
            df['phone'] = df[sorted_phone_cols].apply(get_phone_value, axis=1)
            
            # Drop original phone columns
            df = df.drop(columns=phone_cols)
            print(f"   ðŸ”— Consolidated {len(phone_cols)} phone columns into 'phone'")
        elif len(phone_cols) == 1:
            # Rename single phone column to 'phone'
            df = df.rename(columns={phone_cols[0]: 'phone'})
            print(f"   ðŸ”— Renamed '{phone_cols[0]}' to 'phone'")
        
        # 7. Remove rows that are mostly empty (>90% null)
        row_null_threshold = 0.9
        rows_to_keep = []
        for idx, row in df.iterrows():
            null_ratio = row.isnull().sum() / len(row)
            if null_ratio < row_null_threshold:
                rows_to_keep.append(idx)
        
        removed_rows = len(df) - len(rows_to_keep)
        if removed_rows > 0:
            df = df.loc[rows_to_keep]
            print(f"   âœ‚ï¸  Removed {removed_rows} rows with insufficient data")
        
        # 8. Reorder columns: put important ones first
        priority_cols = ['name', 'phone', 'email', 'address', 'company', 'title', 'notes']
        remaining_cols = [col for col in df.columns if col not in priority_cols]
        ordered_cols = [col for col in priority_cols if col in df.columns] + remaining_cols
        df = df[ordered_cols]
        
        cols_removed = original_cols - len(df.columns)
        rows_removed = original_rows - len(df)
        
        print(f"   âœ… Final: {len(df)} rows Ã— {len(df.columns)} columns")
        print(f"   ðŸ“Š Reduced by {cols_removed} columns and {rows_removed} rows")
        print(f"   ðŸ’¾ Data size reduction: ~{(cols_removed + rows_removed) / (original_cols + original_rows) * 100:.1f}%\n")
        
        return df
    
    def read_csv(self, file_path: str) -> List[Dict[str, Any]]:
        """Read CSV file and return list of documents with phone number fixing"""
        df = pd.read_csv(file_path, encoding='utf-8')
        
        # PREPROCESS: Clean and optimize
        df = self._preprocess_dataframe(df, source_name=Path(file_path).name)
        
        # FIX PHONE NUMBERS BEFORE PROCESSING
        for col in df.columns:
            if 'phone' in col.lower() or 'mobile' in col.lower() or 'tel' in col.lower():
                df[col] = df[col].apply(self._fix_phone_number)
        
        documents = []
        
        for idx, row in df.iterrows():
            text_parts = []
            metadata = {}
            
            for col in df.columns:
                if pd.notna(row[col]):
                    text_parts.append(f"{col}: {row[col]}")
                    metadata[col] = str(row[col])
            
            documents.append({
                'text': ' | '.join(text_parts),
                'metadata': metadata,
                'source': file_path,
                'row_id': idx
            })
        
        return documents
    
    def _fix_phone_number(self, value):
        """Convert scientific notation to proper phone format"""
        if pd.isna(value) or value == '':
            return value
        
        value_str = str(value).strip()
        
        # Detect scientific notation (9.73E+11, 5.42e+08)
        if 'e+' in value_str.lower() or 'e-' in value_str.lower():
            try:
                # Convert to integer
                num = int(float(value_str))
                result = str(num)
                
                # Israeli phone: 9-10 digits, add leading 0 if missing
                if len(result) in [9, 10]:
                    if not result.startswith('0'):
                        result = '0' + result
                
                print(f"Fixed phone: {value_str} â†’ {result}")
                return result
            except (ValueError, OverflowError):
                return value
        
        return value
    
    def read_excel(self, file_path: str) -> List[Dict[str, Any]]:
        """Read Excel file and return list of documents with phone number fixing"""
        workbook = load_workbook(file_path, read_only=True)
        documents = []
        
        for sheet_name in workbook.sheetnames:
            df = pd.read_excel(file_path, sheet_name=sheet_name)
            
            # PREPROCESS: Clean and optimize
            df = self._preprocess_dataframe(df, source_name=f"{Path(file_path).name}/{sheet_name}")
            
            # FIX PHONE NUMBERS BEFORE PROCESSING
            for col in df.columns:
                if 'phone' in col.lower() or 'mobile' in col.lower() or 'tel' in col.lower():
                    df[col] = df[col].apply(self._fix_phone_number)
            
            for idx, row in df.iterrows():
                text_parts = []
                metadata = {'sheet': sheet_name}
                
                for col in df.columns:
                    if pd.notna(row[col]):
                        text_parts.append(f"{col}: {row[col]}")
                        metadata[col] = str(row[col])
                
                documents.append({
                    'text': ' | '.join(text_parts),
                    'metadata': metadata,
                    'source': file_path,
                    'row_id': f"{sheet_name}_{idx}"
                })
        
        return documents
    
    def read_docx(self, file_path: str) -> List[Dict[str, Any]]:
        """Read DOCX file and return list of documents"""
        doc = docx.Document(file_path)
        documents = []
        
        for i, paragraph in enumerate(doc.paragraphs):
            if paragraph.text.strip():
                documents.append({
                    'text': paragraph.text.strip(),
                    'metadata': {'paragraph_id': i},
                    'source': file_path,
                    'row_id': f"para_{i}"
                })
        
        return documents
    
    def read_pdf(self, file_path: str) -> List[Dict[str, Any]]:
        """Read PDF file and return list of documents"""
        documents = []
        
        with open(file_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            
            for page_num, page in enumerate(pdf_reader.pages):
                text = page.extract_text()
                if text.strip():
                    paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
                    
                    for para_idx, paragraph in enumerate(paragraphs):
                        documents.append({
                            'text': paragraph,
                            'metadata': {
                                'page': page_num + 1,
                                'paragraph_id': para_idx
                            },
                            'source': file_path,
                            'row_id': f"page_{page_num}_para_{para_idx}"
                        })
        
        return documents
    
    def read_txt(self, file_path: str) -> List[Dict[str, Any]]:
        """Read TXT file and return list of documents"""
        with open(file_path, 'r', encoding='utf-8') as file:
            content = file.read()
        
        paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
        documents = []
        
        for i, paragraph in enumerate(paragraphs):
            documents.append({
                'text': paragraph,
                'metadata': {'paragraph_id': i},
                'source': file_path,
                'row_id': f"para_{i}"
            })
        
        return documents
    
    def load_file(self, file_path: str) -> List[Dict[str, Any]]:
        """Load file based on extension"""
        file_path = Path(file_path)
        
        if not file_path.exists():
            raise FileNotFoundError(f"File not found: {file_path}")

        extension = file_path.suffix.lower()
        
        print(f"Loading file: {file_path} (type: {extension})")
        
        if extension == '.csv':
            return self.read_csv(str(file_path))
        elif extension in ['.xlsx', '.xls']:
            return self.read_excel(str(file_path))
        elif extension == '.docx':
            return self.read_docx(str(file_path))
        elif extension == '.pdf':
            return self.read_pdf(str(file_path))
        elif extension == '.txt':
            return self.read_txt(str(file_path))
        else:
            raise ValueError(f"Unsupported file format: {extension}")
    
    def ingest_file(self, file_path: str) -> dict:
        """Load a file and add its documents to the database. Returns ingestion report."""
        documents = self.load_file(file_path)
        self.add_documents(documents)
        
        return {
            'documents_added': len(documents),
            'file': Path(file_path).name,
            'total_in_db': self.collection.count()
        }
    
    def add_documents(self, documents: List[Dict[str, Any]]):
        """Add documents to ChromaDB with deterministic IDs; skip existing rows."""
        if not documents:
            print("No documents to add.")
            return

        abs_paths = [str(Path(d['source']).resolve()) for d in documents]
        source_keys = [p.lower() for p in abs_paths]
        ids = [f"{sk}::{d['row_id']}" for sk, d in zip(source_keys, documents)]

        existing = set(self.collection.get(ids=ids).get('ids', []) or [])
        to_add_idx = [i for i, _id in enumerate(ids) if _id not in existing]
        if not to_add_idx:
            print("Nothing new to add (all IDs already present).")
            return

        new_docs = [documents[i] for i in to_add_idx]
        new_ids  = [ids[i]        for i in to_add_idx]

        texts = [doc['text'] for doc in new_docs]
        print(f"Generating embeddings for {len(texts)} new documents...")
        embeddings = self.embedding_model.encode(texts, show_progress_bar=True).tolist()

        metadatas = []
        for doc in new_docs:
            src_path = str(Path(doc['source']).resolve())
            src_name = Path(src_path).name
            src_key  = src_path.lower()

            md = (doc.get('metadata') or {}).copy()
            md.update({
                'source': src_path,
                'source_name': src_name,
                'source_key': src_key,
                'row_id': doc['row_id'],
            })

            stats = self._derive_name_fields(doc['text'], metadata=md)
            md.update(stats)
            metadatas.append(md)

        print(f"Adding to ChromaDB... ({len(new_ids)} new)")
        self.collection.add(
            embeddings=embeddings,
            documents=texts,
            metadatas=metadatas,
            ids=new_ids
        )
        print(f"Successfully added {len(new_ids)} new documents to the vector database!")

    def purge_duplicates(self):
        got = self.collection.get(include=["metadatas"])
        ids   = got.get("ids") or []
        metas = got.get("metadatas") or []
        if not ids or not metas or len(ids) != len(metas):
            print("âš ï¸ Could not read IDs/metadata reliably; skipping dedupe.")
            return
        def _basename(md):
            s = (md.get("source_name") or md.get("source") or "")
            try: return Path(s).name.lower()
            except Exception: return str(s).split("\\")[-1].split("/")[-1].lower()
        seen, to_delete = set(), []
        for _id, md in zip(ids, metas):
            key = (_basename(md), md.get("row_id"))
            if key in seen: to_delete.append(_id)
            else: seen.add(key)
        if to_delete:
            self.collection.delete(ids=to_delete)
            print(f"Deleted {len(to_delete)} duplicate records.")
        else:
            print("No duplicates found.")

    def _fuzzy_text_search(self, query: str, n_results: int = 100, threshold: int = 60) -> Dict[str, Any]:
        """Fuzzy text search - finds similar strings even with typos/variations"""
        from rapidfuzz import fuzz, process
        
        all_data = self.collection.get(include=["documents", "metadatas"])
        documents = all_data.get("documents", [])
        metadatas = all_data.get("metadatas", [])
        ids = all_data.get("ids", [])
        
        if not documents:
            return {'results': []}
        
        matches = []
        for i, (doc, meta, doc_id) in enumerate(zip(documents, metadatas, ids)):
            similarity = fuzz.partial_ratio(query.lower(), doc.lower())
            
            if similarity >= threshold:
                matches.append({
                    'id': doc_id,
                    'document': doc,
                    'metadata': meta,
                    'relevance': similarity / 100.0,
                    'similarity': similarity
                })
        
        matches.sort(key=lambda x: x['relevance'], reverse=True)
        
        return {'results': matches[:n_results]}
    
    def search(self, query: str, n_results: int = 5, similarity_metric: str = "cosine", 
               fuzzy_threshold: float = 80.0, auto_correct: bool = True, 
               hybrid: bool = True) -> Dict[str, Any]:
        """Hybrid search: tries text matching first, then semantic search."""
        original_query = query
        corrected_terms = []
        suggestions = []
        search_method = "hybrid"
        
        if HAS_FUZZY and auto_correct:
            corrected_query, corrections = self._fuzzy_correct_query(query, fuzzy_threshold)
            if corrections:
                corrected_terms = corrections
                query = corrected_query
        elif HAS_FUZZY:
            suggestions = self._get_fuzzy_suggestions(query, fuzzy_threshold)
            if suggestions:
                print(f"\nðŸ’¡ Did you mean: {', '.join([s[0] for s in suggestions[:3]])}?")
        
        text_results = None
        if hybrid and len(query.strip()) >= 2:
            text_results = self._fuzzy_text_search(query, n_results=n_results, threshold=60)
            
            if text_results['results']:
                search_method = "fuzzy_text"
                
                formatted_results = {
                    'query': query,
                    'original_query': original_query,
                    'corrected': corrected_terms,
                    'suggestions': suggestions,
                    'search_method': search_method,
                    'results': []
                }
                
                for result in text_results['results']:
                    formatted_results['results'].append({
                        'id': result['id'],
                        'document': result['document'],
                        'metadata': result['metadata'],
                        'similarity_score': result['relevance'],
                        'match_type': 'text'
                    })
                
                return formatted_results
        
        search_method = "semantic"
        
        query_embedding = self.embedding_model.encode([query])
        
        results = self.collection.query(
            query_embeddings=query_embedding.tolist(),
            n_results=n_results
        )
        
        formatted_results = {
            'query': query,
            'original_query': original_query,
            'corrected': corrected_terms,
            'suggestions': suggestions,
            'search_method': search_method,
            'results': []
        }
        
        if results['documents'] and results['documents'][0]:
            for i in range(len(results['documents'][0])):
                result = {
                    'id': results['ids'][0][i],
                    'document': results['documents'][0][i],
                    'metadata': results['metadatas'][0][i],
                    'distance': results['distances'][0][i],
                    'similarity_score': 1 - results['distances'][0][i],
                    'match_type': 'semantic'
                }
                formatted_results['results'].append(result)
        
        return formatted_results
    
    def search_full_text(self, keyword: str, limit: int = 50) -> list[dict]:
        """Search for keyword across ALL document text with highlighted context"""
        # Common Hebrew profession synonyms
        synonyms = {
            '×¢×•×¨×š ×“×™×Ÿ': ['×¢×•×¨×š ×“×™×Ÿ', '×¢×•"×“', '×¢×•×“', 'lawyer', 'attorney'],
            '×¢×•"×“': ['×¢×•×¨×š ×“×™×Ÿ', '×¢×•"×“', '×¢×•×“', 'lawyer', 'attorney'],
            '×¨×•×¤×': ['×¨×•×¤×', '×“"×¨', '×“×¨', 'doctor', 'dr'],
            '×“×•×§×˜×•×¨': ['×¨×•×¤×', '×“"×¨', '×“×¨', 'doctor', 'dr', '×“×•×§×˜×•×¨'],
        }
        
        # Check if keyword has synonyms
        search_terms = synonyms.get(keyword.strip(), [keyword])
        
        all_data = self.collection.get(include=["documents", "metadatas"])
        documents = all_data.get("documents", [])
        metadatas = all_data.get("metadatas", [])
        
        matches = []
        seen_names = set()  # Avoid duplicates
        
        for search_term in search_terms:
            keyword_norm = self._normalize(search_term).upper()
            
            for doc, meta in zip(documents, metadatas):
                name = meta.get('name', 'Unknown')
                if name in seen_names:
                    continue
                    
                doc_norm = self._normalize(doc).upper()
                if keyword_norm in doc_norm:
                    seen_names.add(name)
                    # Extract context around the keyword
                    context = self._extract_keyword_context(doc, search_term)
                    
                    matches.append({
                        'document': doc,
                        'metadata': meta,
                        'name': name,
                        'keyword_context': context,
                        'matched_term': search_term  # Which variant matched
                    })
                    
                    if len(matches) >= limit:
                        return matches
        
        return matches
    
    def _extract_keyword_context(self, text: str, keyword: str, window: int = 100) -> str:
        """Extract snippet showing where keyword appears in text"""
        keyword_norm = self._normalize(keyword).upper()
        text_norm = self._normalize(text).upper()
        
        # Find keyword position
        pos = text_norm.find(keyword_norm)
        if pos == -1:
            return text[:200]  # Fallback: show beginning
        
        # Extract window around keyword
        start = max(0, pos - window)
        end = min(len(text), pos + len(keyword) + window)
        
        snippet = text[start:end].strip()
        
        # Add ellipsis if truncated
        if start > 0:
            snippet = "..." + snippet
        if end < len(text):
            snippet = snippet + "..."
        
        return snippet
    
    def names_containing(self, substring: str, limit: int = 200) -> list[str]:
        """Find all names containing the substring (case-insensitive)"""
        s = self._normalize(substring)
        names = self.get_all_names()
        out = [nm for nm in names if s.upper() in nm.upper()]
        return sorted(set(out), key=str.upper)[:limit]
    
    def comprehensive_search(self, query: str, max_results: int = 20, show_reasoning: bool = True) -> dict:
        """
        Multi-level search with scoring cascade.
        
        Implements intelligent search hierarchy:
        1. Exact name match (score: 100)
        2. Exact phrase substring (score: 95)
        3. All words present (score: 80)
        4. Partial word match (score: 50 * coverage)
        5. Fuzzy matching (score: similarity * 0.5)
        6. Semantic search (score: similarity * 30)
        
        Returns ranked results with match explanations.
        """
        reasoning_steps = []
        
        # Step 0: Query Analysis
        reasoning_steps.append({
            'step': 0,
            'stage': 'Query Analysis',
            'action': f'Received query: "{query}"',
            'details': 'Analyzing query type and extracting core entity...'
        })
        
        # Extract entity and normalize
        entity = self._extract_entity_from_query(query)
        entity_normalized, entity_no_article = self._normalize_with_article_variants(entity)
        entity_words = [w for w in entity_normalized.split() if w]
        entity_words_no_article = [w for w in entity_no_article.split() if w]
        
        reasoning_steps.append({
            'step': 0.5,
            'stage': 'Entity Extraction',
            'action': f'Extracted entity: "{entity}"',
            'details': f'Normalized: "{entity_normalized}" | Without article: "{entity_no_article}"'
        })
        
        if not entity_words:
            reasoning_steps.append({
                'step': 0.9,
                'stage': 'Validation',
                'action': 'Empty query detected',
                'details': 'Cannot search with empty entity',
                'result': 'âŒ Abort search'
            })
            return {'query': query, 'entity': entity, 'results': [], 'reasoning': reasoning_steps}
        
        # Get all contacts
        all_data = self.collection.get(include=["documents", "metadatas"])
        documents = all_data.get("documents", [])
        metadatas = all_data.get("metadatas", [])
        ids = all_data.get("ids", [])  # IDs are always returned by default
        
        reasoning_steps.append({
            'step': 1,
            'stage': 'Database Load',
            'action': f'Loading contacts from database',
            'details': f'Found {len(documents)} contacts to search through'
        })
        
        if not documents:
            reasoning_steps.append({
                'step': 1.5,
                'stage': 'Database Check',
                'action': 'Database is empty',
                'result': 'âŒ No contacts to search'
            })
            return {'query': query, 'entity': entity, 'results': [], 'reasoning': reasoning_steps}
        
        all_results = []
        seen_ids = set()
        
        # Level tracking for reasoning
        level_results = {
            'exact_name': [],
            'phrase_match': [],
            'all_words': [],
            'partial': [],
            'fuzzy': [],
            'semantic': []
        }
        
        reasoning_steps.append({
            'step': 2,
            'stage': 'Multi-Level Search',
            'action': 'Starting 6-level search cascade',
            'details': 'Level 1: Exact name â†’ Level 2: Phrase match â†’ Level 3: All words â†’ Level 4: Partial â†’ Level 5: Fuzzy â†’ Level 6: Semantic'
        })
        
        for doc, meta, doc_id in zip(documents, metadatas, ids):
            if doc_id in seen_ids:
                continue
            
            name = meta.get('name', '').strip()
            if not name:
                continue
            
            name_normalized, name_no_article = self._normalize_with_article_variants(name)
            doc_normalized, doc_no_article = self._normalize_with_article_variants(doc)
            
            score = 0
            match_methods = []
            
            # Level 1: Exact name match (with OR without article)
            if name_normalized.lower() == entity_normalized.lower() or name_no_article.lower() == entity_no_article.lower():
                score = 100
                match_methods.append("exact_name")
                level_results['exact_name'].append(name)
            
            # Level 2A: Exact phrase substring (check both variants)
            elif (entity_normalized.lower() in doc_normalized.lower() or 
                  entity_no_article.lower() in doc_no_article.lower()):
                score = 95
                match_methods.append("phrase_match")
                level_results['phrase_match'].append(name)
            
            # Level 2B: All words present (check with and without articles)
            elif (all(word in doc_normalized.lower() for word in entity_words) or
                  all(word in doc_no_article.lower() for word in entity_words_no_article)):
                score = 80
                match_methods.append("all_words")
                level_results['all_words'].append(name)
            
            # Level 2C: Partial word match (use version without articles for better matching)
            else:
                words_found = sum(1 for word in entity_words_no_article if word in doc_no_article.lower())
                word_coverage = words_found / len(entity_words_no_article) if entity_words_no_article else 0
                
                if word_coverage > 0:
                    score = 50 * word_coverage
                    match_methods.append(f"partial_{int(word_coverage*100)}%")
                    level_results['partial'].append(f"{name} ({int(word_coverage*100)}%)")
                
                # Level 3: Fuzzy matching
                if score == 0 and HAS_FUZZY:
                    max_similarity = 0
                    for query_word in entity_words_no_article:
                        for doc_word in doc_no_article.lower().split():
                            if len(doc_word) > 2:  # Skip very short words
                                similarity = fuzz.ratio(query_word, doc_word)
                                max_similarity = max(max_similarity, similarity)
                    
                    if max_similarity > 70:
                        score = max_similarity * 0.5  # Scale to max 50
                        match_methods.append(f"fuzzy_{int(max_similarity)}%")
                        level_results['fuzzy'].append(f"{name} ({int(max_similarity)}% similar)")
            
            if score > 0:
                seen_ids.add(doc_id)
                all_results.append({
                    'id': doc_id,
                    'name': name,
                    'document': doc,
                    'metadata': meta,
                    'score': score,
                    'methods': match_methods
                })
        
        # Add reasoning for each level
        reasoning_steps.append({
            'step': 2.1,
            'stage': 'Level 1: Exact Name Match',
            'action': f'Searching for exact name match: "{entity_normalized}"',
            'result': f'âœ… Found {len(level_results["exact_name"])} exact matches' if level_results['exact_name'] else 'âŒ No exact name matches',
            'details': level_results['exact_name'][:3] if level_results['exact_name'] else None
        })
        
        reasoning_steps.append({
            'step': 2.2,
            'stage': 'Level 2: Phrase Match',
            'action': f'Searching for exact phrase in documents',
            'result': f'âœ… Found {len(level_results["phrase_match"])} phrase matches' if level_results['phrase_match'] else 'âŒ No phrase matches',
            'details': level_results['phrase_match'][:3] if level_results['phrase_match'] else None
        })
        
        reasoning_steps.append({
            'step': 2.3,
            'stage': 'Level 3: All Words Present',
            'action': f'Searching for contacts with all query words',
            'result': f'âœ… Found {len(level_results["all_words"])} contacts with all words' if level_results['all_words'] else 'âŒ No contacts with all words',
            'details': level_results['all_words'][:3] if level_results['all_words'] else None
        })
        
        reasoning_steps.append({
            'step': 2.4,
            'stage': 'Level 4: Partial Word Match',
            'action': f'Searching for partial word matches',
            'result': f'âœ… Found {len(level_results["partial"])} partial matches' if level_results['partial'] else 'âŒ No partial matches',
            'details': level_results['partial'][:3] if level_results['partial'] else None
        })
        
        reasoning_steps.append({
            'step': 2.5,
            'stage': 'Level 5: Fuzzy Matching',
            'action': f'Searching for similar words (typo tolerance)',
            'result': f'âœ… Found {len(level_results["fuzzy"])} fuzzy matches' if level_results['fuzzy'] else 'âŒ No fuzzy matches',
            'details': level_results['fuzzy'][:3] if level_results['fuzzy'] else None
        })
        
        # Level 4: Add semantic results (lower scores)
        reasoning_steps.append({
            'step': 2.6,
            'stage': 'Level 6: Semantic Search',
            'action': 'Running AI semantic search for meaning-based matches',
            'details': 'Using sentence transformers to find semantically similar contacts'
        })
        
        semantic_results = self.search(entity, n_results=10)
        semantic_count = 0
        for sem_result in semantic_results.get('results', []):
            result_id = sem_result.get('id')
            if result_id and result_id not in seen_ids:
                semantic_score = sem_result.get('similarity_score', 0) * 30
                seen_ids.add(result_id)
                name = sem_result.get('metadata', {}).get('name', 'Unknown')
                all_results.append({
                    'id': result_id,
                    'name': name,
                    'document': sem_result.get('document', ''),
                    'metadata': sem_result.get('metadata', {}),
                    'score': semantic_score,
                    'methods': [f'semantic_{int(semantic_score)}%']
                })
                level_results['semantic'].append(f"{name} ({int(semantic_score)}%)")
                semantic_count += 1
        
        reasoning_steps.append({
            'step': 2.7,
            'stage': 'Level 6: Semantic Results',
            'result': f'âœ… Found {semantic_count} semantic matches' if semantic_count > 0 else 'âŒ No semantic matches',
            'details': level_results['semantic'][:3] if level_results['semantic'] else None
        })
        
        # Sort by score (highest first)
        all_results.sort(key=lambda x: x['score'], reverse=True)
        
        reasoning_steps.append({
            'step': 3,
            'stage': 'Result Ranking',
            'action': 'Sorting results by confidence score',
            'result': f'âœ… {len(all_results)} total matches found',
            'details': f'Top scores: {[round(r["score"], 1) for r in all_results[:5]]}' if all_results else 'No matches'
        })
        
        reasoning_steps.append({
            'step': 4,
            'stage': 'Final Results',
            'action': f'Returning top {min(max_results, len(all_results))} results',
            'result': 'âœ… Search complete'
        })
        
        return {
            'query': query,
            'entity': entity,
            'total_found': len(all_results),
            'results': all_results[:max_results],
            'reasoning': reasoning_steps,
            'show_reasoning': show_reasoning
        }
        """
        Multi-level search with scoring cascade.
        
        Implements intelligent search hierarchy:
        1. Exact name match (score: 100)
        2. Exact phrase substring (score: 95)
        3. All words present (score: 80)
        4. Partial word match (score: 50 * coverage)
        5. Fuzzy matching (score: similarity * 0.5)
        6. Semantic search (score: similarity * 30)
        
        Returns ranked results with match explanations.
        """
        # Extract entity and normalize
        entity = self._extract_entity_from_query(query)
        entity_normalized, entity_no_article = self._normalize_with_article_variants(entity)
        entity_words = [w for w in entity_normalized.split() if w]
        entity_words_no_article = [w for w in entity_no_article.split() if w]
        
        if not entity_words:
            return {'query': query, 'entity': entity, 'results': []}
        
        # Get all contacts
        all_data = self.collection.get(include=["documents", "metadatas"])
        documents = all_data.get("documents", [])
        metadatas = all_data.get("metadatas", [])
        ids = all_data.get("ids", [])  # IDs are always returned by default
        
        if not documents:
            return {'query': query, 'entity': entity, 'results': []}
        
        all_results = []
        seen_ids = set()
        
        for doc, meta, doc_id in zip(documents, metadatas, ids):
            if doc_id in seen_ids:
                continue
            
            name = meta.get('name', '').strip()
            if not name:
                continue
            
            name_normalized, name_no_article = self._normalize_with_article_variants(name)
            doc_normalized, doc_no_article = self._normalize_with_article_variants(doc)
            
            score = 0
            match_methods = []
            
            # Level 1: Exact name match (with OR without article)
            if name_normalized.lower() == entity_normalized.lower() or name_no_article.lower() == entity_no_article.lower():
                score = 100
                match_methods.append("exact_name")
            
            # Level 2A: Exact phrase substring (check both variants)
            elif (entity_normalized.lower() in doc_normalized.lower() or 
                  entity_no_article.lower() in doc_no_article.lower()):
                score = 95
                match_methods.append("phrase_match")
            
            # Level 2B: All words present (check with and without articles)
            elif (all(word in doc_normalized.lower() for word in entity_words) or
                  all(word in doc_no_article.lower() for word in entity_words_no_article)):
                score = 80
                match_methods.append("all_words")
            
            # Level 2C: Partial word match (use version without articles for better matching)
            else:
                words_found = sum(1 for word in entity_words_no_article if word in doc_no_article.lower())
                word_coverage = words_found / len(entity_words_no_article) if entity_words_no_article else 0
                
                if word_coverage > 0:
                    score = 50 * word_coverage
                    match_methods.append(f"partial_{int(word_coverage*100)}%")
                
                # Level 3: Fuzzy matching
                if score == 0 and HAS_FUZZY:
                    max_similarity = 0
                    for query_word in entity_words_no_article:
                        for doc_word in doc_no_article.lower().split():
                            if len(doc_word) > 2:  # Skip very short words
                                similarity = fuzz.ratio(query_word, doc_word)
                                max_similarity = max(max_similarity, similarity)
                    
                    if max_similarity > 70:
                        score = max_similarity * 0.5  # Scale to max 50
                        match_methods.append(f"fuzzy_{int(max_similarity)}%")
            
            if score > 0:
                seen_ids.add(doc_id)
                all_results.append({
                    'id': doc_id,
                    'name': name,
                    'document': doc,
                    'metadata': meta,
                    'score': score,
                    'methods': match_methods
                })
        
        # Level 4: Add semantic results (lower scores)
        semantic_results = self.search(entity, n_results=10)
        for sem_result in semantic_results.get('results', []):
            result_id = sem_result.get('id')
            if result_id and result_id not in seen_ids:
                semantic_score = sem_result.get('similarity_score', 0) * 30
                seen_ids.add(result_id)
                all_results.append({
                    'id': result_id,
                    'name': sem_result.get('metadata', {}).get('name', 'Unknown'),
                    'document': sem_result.get('document', ''),
                    'metadata': sem_result.get('metadata', {}),
                    'score': semantic_score,
                    'methods': [f'semantic_{int(semantic_score)}%']
                })
        
        # Sort by score (highest first)
        all_results.sort(key=lambda x: x['score'], reverse=True)
        
        return {
            'query': query,
            'entity': entity,
            'total_found': len(all_results),
            'results': all_results[:max_results]
        }
    
    def _extract_entity_from_query(self, query: str) -> str:
        """
        Extract the core entity to search for from a natural language query.
        Handles relationship translations and common patterns.
        """
        query_lower = query.lower().strip()
        
        # Relationship translations (Hebrew)
        relationships = {
            '×ž×™ ×©×™×œ×“×” ××•×ª×™': '××ž×',
            '×ž×™ ×©×”×•×œ×™×“ ××•×ª×™': '××‘×',
            '××ž× ×©×œ×™': '××ž×',
            '××‘× ×©×œ×™': '××‘×',
            '××—×™': '××—×™',
            '××—×•×ª ×©×œ×™': '××—×•×ª',
            '×“×•×“ ×©×œ×™': '×“×•×“',
            '×“×•×“×” ×©×œ×™': '×“×•×“×”',
            '××ž× ×©×œ ××©×ª×™': '×—×ž×•×ª×™',
            '××‘× ×©×œ ××©×ª×™': '×—×ž×™',
            'my mother': '××ž×',
            'my father': '××‘×',
            'my brother': '××—×™',
            'my sister': '××—×•×ª',
            'my uncle': '×“×•×“',
            'my aunt': '×“×•×“×”',
        }
        
        # Check for relationship phrases
        for phrase, term in relationships.items():
            if phrase in query_lower:
                return term
        
        # Remove common query words
        removal_patterns = [
            r'×”×˜×œ×¤×•×Ÿ ×©×œ ',
            r'×˜×œ×¤×•×Ÿ ×©×œ ',
            r'×ž×¡×¤×¨ ×©×œ ',
            r'phone of ',
            r'number of ',
            r"i'd like ",
            r"i want ",
            r'the number of ',
            r'the phone of ',
            r'×›×œ ×ž×™ ×©',
            r'×›×œ ×”',
            r'all the ',
            r'all ',
        ]
        
        cleaned = query_lower
        for pattern in removal_patterns:
            cleaned = re.sub(pattern, '', cleaned, flags=re.IGNORECASE)
        
        return cleaned.strip() or query
    
    def get_collection_stats(self) -> Dict[str, Any]:
        """Get statistics about the collection"""
        count = self.collection.count()
        return {
            'document_count': count,
            'collection_name': self.collection_name,
            'embedding_model': self.model_name
        }
    
    def list_by_prefix(self, letter: str) -> list[str]:
        letter = letter.upper().strip()
        all_docs = self.collection.get(include=["metadatas"])
        names = []
        for md in all_docs.get("metadatas", []):
            name = (md.get("name") or "").strip()
            if not name:
                continue
            # Get first alphabetic character
            first_char = None
            for ch in name:
                if ch.isalpha():
                    first_char = ch.upper()
                    break
            if first_char and first_char == letter:
                names.append(name)
        return sorted(set(names), key=str.upper)

    def _build_vocabulary(self) -> set:
        """Build vocabulary from all documents in the database."""
        all_docs = self.collection.get(include=["documents"])
        documents = all_docs.get("documents", [])
        
        vocabulary = set()
        for doc in documents:
            words = re.findall(r'\b[a-zA-Z0-9]+\b', doc.lower())
            vocabulary.update(words)
        
        return vocabulary
    
    def _fuzzy_correct_query(self, query: str, threshold: float = 80.0) -> Tuple[str, List[Tuple[str, str]]]:
        """Correct typos in query using fuzzy matching against database vocabulary."""
        if not HAS_FUZZY:
            return query, []
        
        vocabulary = self._build_vocabulary()
        
        if not vocabulary:
            return query, []
        
        query_words = query.split()
        corrected_words = []
        corrections = []
        
        for word in query_words:
            word_lower = word.lower()
            
            if word_lower in vocabulary:
                corrected_words.append(word)
                continue
            
            matches = process.extract(
                word_lower,
                vocabulary,
                scorer=fuzz.ratio,
                limit=1
            )
            
            if matches and matches[0][1] >= threshold:
                best_match = matches[0][0]
                if word[0].isupper():
                    best_match = best_match.capitalize()
                corrected_words.append(best_match)
                corrections.append((word, best_match))
            else:
                corrected_words.append(word)
        
        corrected_query = " ".join(corrected_words)
        return corrected_query, corrections
    
    def _get_fuzzy_suggestions(self, query: str, threshold: float = 70.0) -> List[Tuple[str, float]]:
        """Get fuzzy match suggestions for query terms without auto-correcting."""
        if not HAS_FUZZY:
            return []
        
        vocabulary = self._build_vocabulary()
        if not vocabulary:
            return []
        
        all_suggestions = []
        for word in query.split():
            word_lower = word.lower()
            if word_lower not in vocabulary:
                matches = process.extract(
                    word_lower,
                    vocabulary,
                    scorer=fuzz.ratio,
                    limit=3
                )
                for match_word, score in matches:
                    if score >= threshold:
                        all_suggestions.append((match_word, score))
        
        return sorted(all_suggestions, key=lambda x: x[1], reverse=True)[:5]
    
    def _normalize(self, s: str) -> str:
        """Normalize text for search - handles Hebrew Unicode properly"""
        if not s:
            return ""
        
        # First apply NFKC normalization
        s = unicodedata.normalize("NFKC", s)
        
        # Remove Hebrew vowel points (nikud) and cantillation marks
        # Range U+0591-U+05C7 covers most Hebrew diacritics
        s = re.sub(r'[\u0591-\u05C7]', '', s)
        
        # Remove zero-width characters and soft hyphens
        s = re.sub(r'[\u200B-\u200D\uFEFF\u00AD]', '', s)
        
        # Normalize whitespace
        s = re.sub(r'\s+', ' ', s)
        
        return s.strip()
    
    def _normalize_with_article_variants(self, s: str) -> tuple[str, str]:
        """
        Normalize text and also return version without Hebrew definite article.
        Useful for matching "\u05d1\u05d9\u05ea" (house) with "\u05d4\u05d1\u05d9\u05ea" (the house).
        
        Returns: (normalized, normalized_without_article)
        """
        normalized = self._normalize(s)
        
        # Remove Hebrew definite article "\u05d4" at word boundaries
        # Pattern: space + \u05d4 + letter -> space + letter
        without_article = re.sub(r'\s+\u05d4([\u05d0-\u05ea])', r' \1', normalized)
        # Also handle start of string
        without_article = re.sub(r'^\u05d4([\u05d0-\u05ea])', r'\1', without_article)
        
        return normalized, without_article
    
    def _derive_name_fields(self, text: str, metadata: Optional[dict] = None) -> dict:
        name = ""
        md = metadata or {}
        
        # Try First + Last Name (Google Contacts CSV)
        first = md.get('First Name', '').strip()
        last = md.get('Last Name', '').strip()
        
        if first or last:
            name = ' '.join([p for p in [first, last] if p])
        
        # Fallback to Organization Name
        if not name:
            name = md.get('Organization Name', '').strip()
        
        # Last resort: extract from text
        if not name:
            preferred_keys = ['name', 'title', 'industry_name']
            for k in preferred_keys:
                v = md.get(k)
                if v and str(v).strip():
                    name = str(v).strip()
                    break
        
        if not name:
            t = self._normalize(text)
            first_chunk = t.split(" | ")[0]
            if ":" in first_chunk:
                maybe = first_chunk.split(":", 1)[1].strip()
                if maybe:
                    name = maybe
            if not name:
                name = first_chunk.strip()
        
        first_char = ""
        for ch in name:
            if ch.isalpha():
                first_char = ch.upper() if ch.isascii() else ch
                break
        
        return {
            "name": name,
            "first_char": first_char,
            "name_len": len(name),
        }
    def _get_all_metadatas(self) -> List[dict]:
        out = self.collection.get(include=["metadatas"])
        return out.get("metadatas", []) or []

    def get_all_names(self) -> List[str]:
        names = []
        for md in self._get_all_metadatas():
            name = self._normalize(md.get("name") or "")
            if name:
                names.append(name)
        return sorted(set(names), key=str.upper)

    def letter_histogram(self) -> Dict[str, int]:
        hist = {}
        for md in self._get_all_metadatas():
            ch = md.get("first_char") or ""
            if ch:
                hist[ch] = hist.get(ch, 0) + 1
        return dict(sorted(hist.items(), key=lambda kv: kv[0]))

    def length_histogram(self) -> Dict[int, int]:
        hist = {}
        for md in self._get_all_metadatas():
            n = int(md.get("name_len") or 0)
            hist[n] = hist.get(n, 0) + 1
        return dict(sorted(hist.items()))

    def first_n_by_prefix(self, prefix: str, n: int = 5) -> List[str]:
        p = self._normalize(prefix)
        names = self.get_all_names()
        return sorted([nm for nm in names if nm.upper().startswith(p.upper())], key=str.upper)[:n]

    def count_by_prefix(self, prefix: str) -> int:
        p = self._normalize(prefix)
        return sum(1 for md in self._get_all_metadatas()
                if (md.get("name") or "").upper().startswith(p.upper()))
    
    def _clean_for_len(self, s: str) -> str:
        return re.sub(r"[^0-9A-Za-z\u0590-\u05FF]", "", s)

    def _measure_len(self, name: str, count_mode: str = "chars") -> int:
        if count_mode == "letters":
            return len(self._clean_for_len(name))
        if count_mode == "words":
            return len([w for w in re.split(r"\s+", name.strip()) if w])
        return len(name)
    
    def names_by_length(self, length: int, limit: int = 200, count_mode="chars") -> List[str]:
        names = self.get_all_names()
        out = [nm for nm in names if self._measure_len(nm, count_mode) == length]
        return sorted(out, key=str.upper)[:limit]
    
    def names_by_prefix_and_length(self, prefix: str, length: int, limit: int = 200, count_mode="chars") -> list[str]:
        p = self._normalize(prefix)
        L = int(length)
        names = self.get_all_names()
        out = [nm for nm in names if nm.upper().startswith(p.upper()) and self._measure_len(nm, count_mode) == L]
        return sorted(set(out), key=str.upper)[:limit]

    def list_sources(self) -> Dict[str, int]:
        """Return {source_name: count} with canonical grouping."""
        out = self.collection.get(include=['metadatas'])
        metas = out.get('metadatas', []) or []
        counts = Counter()
        for md in metas:
            name = md.get('source_name') or md.get('source') or 'UNKNOWN'
            key  = md.get('source_key') or (md.get('source') or '').lower()
            counts[(key, name)] += 1
        display = Counter()
        for (_, name), c in counts.items():
            display[name] += c
        return dict(display)

    def count_sources(self) -> int:
        """Number of distinct canonical sources (by source_key)."""
        out = self.collection.get(include=['metadatas'])
        metas = out.get('metadatas', []) or []
        keys = {md.get('source_key') or (md.get('source') or '').lower() for md in metas if (md.get('source') or '')}
        return len(keys)


class AdvancedVectorDBQASystem(VectorDBQASystem):
    """LangChain-powered agent system"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise RuntimeError("OPENAI_API_KEY is missing. Set it in your environment or .env file.")
        
        self.api_key = api_key
        self.llm = ChatOpenAI(model="gpt-4o-mini", api_key=api_key, temperature=0)
        
        # Conversation memory (LangChain format)
        self.chat_history = []
        self.max_history_turns = 5  # Reduced from 10 for performance

    def _create_tools(self):
        """Create LangChain tools with @tool decorator"""
        
        # Store reference to self for tools to use
        qa_system = self
        
        @tool
        def count_documents() -> dict:
            """Get the total number of contacts/documents in the database.
            This is the authoritative count - use this when user asks 'how many contacts'."""
            total = qa_system.collection.count()
            return {
                "total_contacts": total,
                "message": f"There are {total:,} contacts in the database."
            }
        
        @tool
        def count_by_prefix(letter: str) -> dict:
            """Count contacts starting with a specific letter."""
            count = qa_system.count_by_prefix(letter)
            return {"letter": letter, "count": count}
        
        @tool
        def count_by_language() -> dict:
            """Count how many contacts have Hebrew names vs English/Latin names.
            Analyzes the first character of each contact name to determine language."""
            all_metas = qa_system._get_all_metadatas()
            
            hebrew_count = 0
            english_count = 0
            other_count = 0
            
            for meta in all_metas:
                name = meta.get('name', '').strip()
                if not name:
                    other_count += 1
                    continue
                
                # Get first alphabetic character
                first_char = None
                for ch in name:
                    if ch.isalpha():
                        first_char = ch
                        break
                
                if not first_char:
                    other_count += 1
                    continue
                
                # Check if Hebrew (U+0590 to U+05FF)
                if '\u0590' <= first_char <= '\u05FF':
                    hebrew_count += 1
                # Check if English/Latin (A-Z, a-z)
                elif first_char.isascii() and first_char.isalpha():
                    english_count += 1
                else:
                    other_count += 1
            
            return {
                'total': len(all_metas),
                'hebrew': hebrew_count,
                'english': english_count,
                'other': other_count
            }
        
        @tool
        def smart_search(query: str, max_results: int = 20) -> dict:
            """ðŸŒŸ PRIMARY SEARCH TOOL - Use this for ALL person/contact searches!
            
            Intelligent multi-level search with automatic fallback cascade:
            1. Exact name match (100% confidence)
            2. Exact phrase match (95% confidence)
            3. All query words present (80% confidence)
            4. Partial word matches (50%+ confidence)
            5. Fuzzy/typo matching (40-50% confidence)
            6. Semantic/meaning search (0-30% confidence)
            
            Automatically:
            - Translates relationship queries ("×ž×™ ×©×™×œ×“×” ××•×ª×™" â†’ "××ž×")
            - Handles typos and variations
            - Returns ALL relevant results ranked by confidence
            - Shows why each result matched
            
            Use for:
            - Individual person searches ("×“×•×“", "David")
            - Relationship queries ("××ž× ×©×œ×™", "my uncle")
            - Names with typos ("curtain" â†’ finds "curtains")
            - Any natural language person search
            
            Returns: Ranked list with scores and match explanations.
            """
            results = qa_system.comprehensive_search(query, max_results=max_results)
            
            # Format for agent
            formatted = {
                'query': results['query'],
                'entity_searched': results['entity'],
                'total_found': results['total_found'],
                'contacts': []
            }
            
            for r in results['results']:
                meta = r['metadata']
                phone = meta.get('phone', 'N/A')
                
                # Try to extract phone from other fields
                if phone == 'N/A':
                    for field in meta:
                        if 'phone' in field.lower():
                            phone = meta[field]
                            break
                
                formatted['contacts'].append({
                    'name': r['name'],
                    'phone': phone,
                    'score': round(r['score'], 1),
                    'match_type': ', '.join(r['methods']),
                    'metadata': meta
                })
            
            return formatted
        
        @tool
        def search(query: str, n_results: int = 5) -> dict:
            """Semantic search over the vector DB. Returns top 5 most relevant documents by default.
            Use this for fuzzy/similarity search, Hebrew names, relationships, or when exact match fails.
            For finding ALL matches or many results, increase n_results (e.g., n_results=20)."""
            return qa_system.search(query, n_results=n_results)
        
        @tool
        def search_keyword(keyword: str, limit: int = 50) -> dict:
            """Search for exact keyword across all contact data (case-insensitive).
            Returns structured results with name, phone, and WHERE the keyword was found.
            When user says 'all' or 'anyone', increase limit to 100."""
            results = qa_system.search_full_text(keyword, limit=limit)
            
            # FORCE structured output that GPT-5 must display
            formatted = []
            for r in results:
                meta = r['metadata']
                
                # Extract phone (try multiple field names) and format properly
                phone = None
                for field in meta:
                    if 'phone' in field.lower() and 'value' in field:
                        phone_raw = meta[field]
                        if phone_raw:
                            # Convert to string and remove scientific notation
                            phone = str(phone_raw).replace('.0', '')
                            # Format as phone number
                            if phone and phone != 'nan' and len(phone) > 5:
                                break
                        phone = None
                
                # Extract email
                email = None
                for field in meta:
                    if 'mail' in field.lower() and 'value' in field:
                        email = meta[field]
                        if email and str(email).strip() and str(email) != 'nan':
                            break
                        email = None
                
                # CRITICAL: Create a display string that FORCES context to show
                context_snippet = r['keyword_context'][:120].strip()
                display_line = f"{r['name']} | Phone: {phone or 'N/A'}"
                if context_snippet and keyword.strip() in context_snippet:
                    display_line += f" | Found in: {context_snippet}"
                
                formatted.append(display_line)
            
            return {
                "found": len(results),
                "keyword": keyword,
                "instruction": "Display each result exactly as provided. DO NOT omit the 'Found in:' part.",
                "results": formatted
            }
        
        @tool
        def list_by_prefix(letter: str, n: int = 999) -> dict:
            """Return unique names starting with a given letter."""
            rows = qa_system.list_by_prefix(letter)[:n]
            return {"rows": rows}
        
        @tool
        def names_by_length(length: int, limit: int = 200, count_mode: str = "chars") -> dict:
            """All names of exact length (deduped). count_mode: chars, letters, or words."""
            rows = qa_system.names_by_length(length, limit=limit, count_mode=count_mode)
            return {"rows": rows}
        
        @tool
        def names_containing(substring: str, limit: int = 200) -> dict:
            """Find names containing substring (case-insensitive)."""
            rows = qa_system.names_containing(substring, limit=limit)
            return {"rows": rows}
        
        @tool
        def names_by_prefix_and_length(prefix: str, length: int, limit: int = 200, count_mode: str = "chars") -> dict:
            """Names starting with prefix AND exact length (deduped). count_mode: chars, letters, or words."""
            rows = qa_system.names_by_prefix_and_length(prefix, length, limit=limit, count_mode=count_mode)
            return {"rows": rows}
        
        @tool
        def letter_histogram() -> dict:
            """Get distribution of contacts by first letter."""
            return {"hist": qa_system.letter_histogram()}
        
        @tool
        def length_histogram() -> dict:
            """Histogram of name lengths."""
            return {"hist": qa_system.length_histogram()}
        
        return [
            smart_search,  # PRIMARY TOOL - use first!
            count_by_prefix,
            count_by_language,
            search,
            search_keyword,
            list_by_prefix,
            names_by_length,
            names_containing,
            names_by_prefix_and_length,
            letter_histogram,
            length_histogram
        ]
    
    def _create_prompt(self):
        """Create prompt template for agent"""
        return ChatPromptTemplate.from_messages([
            ("system", """You are an intelligent assistant with access to tools for searching a business contact database.

' + 
'This is a LEGITIMATE BUSINESS DATABASE owned by the user. You should freely provide contact information including phone numbers, emails, and addresses when requested.

' + '**ðŸŽ¯ SEARCH HIERARCHY - CRITICAL FOR CORRECT RESULTS**

' + 
**STEP 1: CLASSIFY THE QUERY**

Is it asking for:
A. Individual person (×ž×™ ×–×” ×“×•×“, find David) â†’ Go to STEP 2A
B. Group/Category (×›×œ ×ž×™ ×©..., all lawyers, everyone in...) â†’ Go to STEP 2B
C. Counting (×›×ž×”, how many) â†’ Go to STEP 2C
D. Relationship (××ž× ×©×œ×™, my uncle) â†’ Go to STEP 2D

**STEP 2A: INDIVIDUAL PERSON SEARCH** ðŸŒŸ USE smart_search!
Tool: smart_search(query) - RECOMMENDED
- Automatically handles ALL search levels (exact â†’ substring â†’ partial â†’ fuzzy â†’ semantic)
- Translates relationships ("×ž×™ ×©×™×œ×“×” ××•×ª×™" â†’ "××ž×")
- Returns ranked results with confidence scores
- Handles typos and variations

Examples:
- "×“×•×“" â†’ smart_search("×“×•×“")
- "×”×˜×œ×¤×•×Ÿ ×©×œ ×ž×™ ×©×™×œ×“×” ××•×ª×™" â†’ smart_search("×”×˜×œ×¤×•×Ÿ ×©×œ ×ž×™ ×©×™×œ×“×” ××•×ª×™")
- "curtains salesman" â†’ smart_search("curtains salesman") â† finds "curtain", "curtains", "Chen curtains"

**STEP 2B: GROUP/CATEGORY SEARCH** â­ MOST IMPORTANT

Detect if query contains: ×›×œ, ×›×•×œ×, all, everyone, list, who are the

Then identify type:

1. **ROLE/PROFESSION/KEYWORD** (Most common!)
   Keywords: ×¢×•×¨×š ×“×™×Ÿ, ×¨×•×¤×, ×˜×¨×ž×¤, ×•×¢×“ ×‘×™×ª, ×©×¨×‘×¨×‘, ×—×©×ž×œ××™, ×ž×©×’×™×—
   
   **CRITICAL PROCESS:**
   Step 1: Extract the keyword (e.g., "×•×¢×“ ×‘×™×ª" from "×›×œ ×ž×™ ×©×‘×•×•×¢×“ ×‘×™×ª")
   Step 2: **ALWAYS** try search_keyword FIRST with limit=100
   Step 3: If <3 results â†’ try search with n_results=20 as fallback
   Step 4: Return ALL results (don't truncate to 2-3!)
   
   Examples:
   - "×›×œ ×ž×™ ×©×‘×•×•×¢×“ ×‘×™×ª" â†’ search_keyword("×•×¢×“ ×‘×™×ª", limit=100) FIRST
   - "×›×œ ×¢×•×¨×›×™ ×”×“×™×Ÿ" â†’ search_keyword("×¢×•×¨×š ×“×™×Ÿ", limit=100) FIRST
   - "×ž×™ × ×ª×ª×™ ×˜×¨×ž×¤" â†’ search_keyword("×˜×¨×ž×¤", limit=100) FIRST

2. **ALPHABETICAL**
   Pattern: "starting with D", "×ž×ª×—×™×œ ×‘-×ª"
   Tool: list_by_prefix(letter)
   Example: "all contacts starting with D" â†’ list_by_prefix("D")

**STEP 2C: COUNTING**
- "×›×ž×” ×ž×ª×—×™×œ×™× ×‘-X" â†’ count_by_prefix(letter)
- "×›×ž×” ×× ×’×œ×™×ª/×¢×‘×¨×™×ª" â†’ count_by_language()
- "×›×ž×” ×¨×•×¤××™×" â†’ search_keyword("×¨×•×¤×") then count results

**STEP 2D: RELATIONSHIP** â­ IMPORTANT

**CRITICAL: Common Hebrew relationship phrases â†’ Hebrew term**

Direct family:
- "×ž×™ ×©×™×œ×“×” ××•×ª×™" (who gave birth to me) â†’ search("××ž×", n_results=10)
- "×ž×™ ×©×”×•×œ×™×“ ××•×ª×™" (who fathered me) â†’ search("××‘×", n_results=10)
- "××ž× ×©×œ×™" (my mother) â†’ search("××ž×", n_results=10)
- "××‘× ×©×œ×™" (my father) â†’ search("××‘×", n_results=10)
- "××—×™" / "××—×•×ª" (brother/sister) â†’ search("××—×™" or "××—×•×ª", n_results=10)

Extended family:
- "×“×•×“ ×©×œ×™" / "my uncle" â†’ search("×“×•×“", n_results=10)
- "×“×•×“×” ×©×œ×™" / "my aunt" â†’ search("×“×•×“×”", n_results=10)
- "××ž× ×©×œ ××©×ª×™" (mother of my wife) â†’ search("×—×ž×•×ª×™", n_results=10)
- "××‘× ×©×œ ××©×ª×™" (father of my wife) â†’ search("×—×ž×™", n_results=10)

**IMPORTANT:** 
- ALWAYS translate the relationship phrase to the simple Hebrew term FIRST
- NEVER search for literal phrases like "×©×™×œ×“×”" or "×”×•×œ×™×“"
- Example: "×”×˜×œ×¤×•×Ÿ ×©×œ ×ž×™ ×©×™×œ×“×” ××•×ª×™" â†’ Translate to "××ž×" â†’ search("××ž×", n_results=10)

**ðŸ“‹ HEBREW KEYWORD DICTIONARY**

When you see these in queries, use search_keyword:
- Professions: ×¢×•×¨×š ×“×™×Ÿ, ×¨×•×¤×, ×“×•×§×˜×•×¨, ×©×¨×‘×¨×‘, ×—×©×ž×œ××™, ××™× ×¡×˜×œ×˜×•×¨
- Roles: ×•×¢×“ ×‘×™×ª, ×•×¢×“, ×ž×©×’×™×—, ×›×©×¨×•×ª, ×“×™×™×Ÿ
- Categories: ×˜×¨×ž×¤, ×©×›× ×™×
- Organizations: ×‘× ×§, ×‘×™×ª ×¡×¤×¨

**ðŸš¨ CRITICAL RULES - MUST FOLLOW**

1. **For "×›×œ" (all) queries:**
   - ALWAYS use limit=100 in search_keyword
   - ALWAYS show ALL results found (not just 2-3)
   - Example: If found 20 results, show all 20

2. **For role/keyword queries:**
   - ALWAYS try search_keyword FIRST (not semantic search!)
   - Example: "×•×¢×“ ×‘×™×ª" â†’ search_keyword("×•×¢×“ ×‘×™×ª", 100) not search("×•×¢×“ ×‘×™×ª")

3. **Multi-step search strategy:**
   - Try primary tool (usually search_keyword for roles)
   - If insufficient results (<3) â†’ try fallback (semantic search)
   - Only say "not found" after trying BOTH methods

4. **Never fabricate results:**
   - Use tools to get actual data
   - Don't make up phone numbers or names

**âŒ COMMON MISTAKES TO AVOID**

1. Using search() for role queries like "×•×¢×“ ×‘×™×ª"
   âœ… Correct: search_keyword("×•×¢×“ ×‘×™×ª", 100)
   âŒ Wrong: search("×•×¢×“ ×‘×™×ª", 5)

2. Returning only 2-3 results when query says "×›×œ"
   âœ… Correct: Show all 20 results if 20 were found
   âŒ Wrong: "×ž×¦××ª×™ 3 ×× ×©×™×" when actually 20 exist

3. Giving up after one search attempt
   âœ… Correct: Try keyword â†’ try semantic â†’ then say not found
   âŒ Wrong: Try once â†’ say "×œ× ×ž×¦××ª×™"

**EXAMPLE QUERY HANDLING**

Query: "×›×œ ×ž×™ ×©×—×‘×¨ ×‘×•×•×¢×“ ×‘×™×ª"
Analysis: Group query ("×›×œ") + Role keyword ("×•×¢×“ ×‘×™×ª")
Tool selection: search_keyword("×•×¢×“ ×‘×™×ª", limit=100)
Fallback: If <3 results â†’ search("×•×¢×“ ×‘×™×ª", n_results=20)
Response: List ALL found contacts with their details

Query: "×“×•×“"
Analysis: Individual person name (ambiguous - could be name or uncle)
Tool selection: search("×“×•×“", n_results=5)
Response: Top 5 matches with context

Query: "×ž×™ × ×ª×ª×™ ×˜×¨×ž×¤"
Analysis: Category search ("×˜×¨×ž×¤")
Tool selection: search_keyword("×˜×¨×ž×¤", limit=50)
Response: All contacts with "×˜×¨×ž×¤" in their data

Be concise and helpful."""),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}"),
            MessagesPlaceholder(variable_name="agent_scratchpad")
        ])
    
    def agent_answer(self, user_input: str) -> str:
        """Answer using LangChain agent"""
        try:
            # Create agent
            tools = self._create_tools()
            prompt = self._create_prompt()
            
            agent = create_openai_functions_agent(
                llm=self.llm,
                tools=tools,
                prompt=prompt
            )
            
            agent_executor = AgentExecutor(
                agent=agent,
                tools=tools,
                verbose=False,
                max_iterations=3,  # Reduced from 5 for faster responses
                handle_parsing_errors=True
            )
            
            # Run agent with chat history
            response = agent_executor.invoke({
                "input": user_input,
                "chat_history": self.chat_history
            })
            
            # Update chat history
            self.chat_history.append(HumanMessage(content=user_input))
            self.chat_history.append(AIMessage(content=response['output']))
            
            # Trim history if too long
            self._trim_history()
            
            return response['output']
            
        except Exception as e:
            error_msg = f"Error: {str(e)}"
            print(error_msg)
            return error_msg
    
    def _trim_history(self):
        """Keep only recent conversation turns"""
        # Each turn = 2 messages (human + AI)
        max_messages = self.max_history_turns * 2
        
        if len(self.chat_history) > max_messages:
            # Keep only recent messages (silently trim)
            self.chat_history = self.chat_history[-max_messages:]
    
    def _clear_history(self):
        """Clear conversation history"""
        self.chat_history = []
        print("Conversation history cleared. Starting fresh!")
    
    def _show_history(self):
        """Display conversation history"""
        if not self.chat_history:
            print("No conversation history yet. Ask me something!")
            return
        
        print("\n" + "="*60)
        print("CONVERSATION HISTORY")
        print("="*60)
        
        turn = 0
        for i in range(0, len(self.chat_history), 2):
            turn += 1
            print(f"\n--- Turn {turn} ---")
            
            if i < len(self.chat_history):
                human_msg = self.chat_history[i].content
                print(f"Human: {human_msg[:200]}{'...' if len(human_msg) > 200 else ''}")
            
            if i + 1 < len(self.chat_history):
                ai_msg = self.chat_history[i + 1].content
                print(f"AI: {ai_msg[:200]}{'...' if len(ai_msg) > 200 else ''}")
        
        print("\n" + "="*60)
        print(f"Total turns: {turn} | Messages in memory: {len(self.chat_history)}")
        print("="*60)
    
    def _is_exit_intent(self, text: str) -> bool:
        """Detect if user wants to exit (hidden detection)"""
        exit_phrases = [
            'quit', 'exit', 'q', 'bye', 'goodbye',
            '×—×œ××¡', '×¡×™×•×', '×œ×”×ª×¨××•×ª', '×™×¦×™××”', '×¡×’×•×¨', '×‘×™×™',
            '×ª×•×“×” ×•×œ×”×ª×¨××•×ª', '×“×™', '×“×™ ×ª×•×“×”', '×–×”×•'
        ]
        return text.lower().strip() in exit_phrases
    
    def _is_stats_request(self, text: str) -> bool:
        """Detect if user wants database statistics"""
        stats_keywords = [
            'stats', 'statistics', '×ž×™×“×¢', '×¡×˜×˜×™×¡×˜×™×§×”',
            '×›×ž×” ×× ×©×™×', '×›×ž×” ××™×©', '×›×ž×” ×¨×©×•×ž×•×ª', '×›×ž×” ×ž×¡×ž×›×™×',
            'how many', 'database info', 'db info', '×ž×¦×‘ ×”×ž××’×¨'
        ]
        text_lower = text.lower().strip()
        return any(keyword in text_lower for keyword in stats_keywords)
    
    def _is_history_request(self, text: str) -> bool:
        """Detect if user wants conversation history"""
        history_keywords = [
            'history', '×”×™×¡×˜×•×¨×™×”', '×”×¦×’ ×”×™×¡×˜×•×¨×™×”', '×”×¨××” ×”×™×¡×˜×•×¨×™×”',
            'show history', 'conversation history', 'chat history',
            '×ž×” ×“×™×‘×¨× ×•', '×¢×œ ×ž×” ×“×™×‘×¨× ×•', '×”×©×™×—×•×ª ×©×œ× ×•'
        ]
        text_lower = text.lower().strip()
        return any(keyword in text_lower for keyword in history_keywords)
    
    def _is_clear_request(self, text: str) -> bool:
        """Detect if user wants to clear history"""
        clear_keywords = [
            'clear', 'reset', '× ×§×”', '××¤×¡', '×”×ª×—×œ ×ž×—×“×©',
            'clear history', 'reset conversation', 'start over',
            '×©×›×—', '×©×›×— ×”×›×œ', '×ž×—×§ ×”×™×¡×˜×•×¨×™×”'
        ]
        text_lower = text.lower().strip()
        return any(keyword in text_lower for keyword in clear_keywords)
    
    def _is_load_request(self, text: str) -> tuple[bool, str]:
        """Detect if user wants to load a file, return (is_load, filepath)"""
        text_lower = text.lower().strip()
        if text_lower.startswith('load '):
            return True, text[5:].strip()
        load_patterns = ['×˜×¢×Ÿ', '×”×¢×œ×”', '×”×•×¡×£ ×§×•×‘×¥', 'load file', 'add file']
        if any(pattern in text_lower for pattern in load_patterns):
            # Try to extract filepath
            words = text.split()
            for word in words:
                if '.' in word or '/' in word:
                    return True, word
        return False, ""
    
    def interactive_qa(self):
        """Interactive Q&A session with natural language understanding"""
        print("\n")
        
        while True:
            try:
                user_input = input("\nEnter your query or command: ").strip()
                
                if not user_input:
                    continue
                
                # Natural language intent detection
                if self._is_exit_intent(user_input):
                    print("×œ×”×ª×¨××•×ª! Goodbye!")
                    break
                
                elif self._is_stats_request(user_input):
                    stats = self.get_collection_stats()
                    print(f"\n×ž×™×“×¢ ×¢×œ ×”×ž××’×¨ / Database Statistics:")
                    print(f"   ×ž×¡×ž×›×™× / Documents: {stats['document_count']}")
                    print(f"   ×ž×•×“×œ / Model: {stats['embedding_model']}")
                    print(f"   ×§×•×œ×§×¦×™×” / Collection: {stats['collection_name']}")
                
                elif self._is_history_request(user_input):
                    self._show_history()
                
                elif self._is_clear_request(user_input):
                    self._clear_history()
                
                else:
                    # Check for load request
                    is_load, filepath = self._is_load_request(user_input)
                    if is_load and filepath:
                        try:
                            documents = self.load_file(filepath)
                            self.add_documents(documents)
                        except Exception as e:
                            print(f"×©×’×™××” ×‘×˜×¢×™× ×ª ×”×§×•×‘×¥ / Error loading file: {e}")
                    else:
                        # Regular query - use agent
                        if self.collection.count() == 0:
                            print("××™×Ÿ ×ž×¡×ž×›×™× ×‘×ž××’×¨. ×˜×¢×Ÿ ×§×•×‘×¥ ×ª×—×™×œ×”. / No documents in database. Load a file first.")
                            continue
                        
                        response = self.agent_answer(user_input)
                        print(f"\n{response}\n")
         
            except KeyboardInterrupt:
                print("\n\n×œ×”×ª×¨××•×ª! Goodbye!")
                break
            except Exception as e:
                print(f"×©×’×™××” / Error: {e}")


def main():
    """Main function"""
    print("\n" + "="*50)
    print("  VectorDB Q&A System")
    print("  ×ž×¢×¨×›×ª ×©××œ×•×ª ×•×ª×©×•×‘×•×ª ×ž×‘×•×¡×¡×ª AI")
    print("="*50 + "\n")
    
    qa_system = AdvancedVectorDBQASystem()
    qa_system.interactive_qa()


if __name__ == "__main__":
    if not os.getenv("OPENAI_API_KEY"):
        raise RuntimeError("Missing OPENAI_API_KEY. Create a .env or set the env var.")
    main()
